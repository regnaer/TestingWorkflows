name: Fetch All Project Items (Paginated)

on:
  workflow_dispatch:
    inputs:
      project_node_id:
        description: 'The Node ID of the ProjectV2 board (e.g., PVT_...)'
        required: true
        type: string
      output_filename:
        description: 'Filename for the final JSON output'
        required: false
        type: string
        default: 'all_project_items.json'

jobs:
  fetch-all-items:
    runs-on: ubuntu-latest
    permissions:
      contents: read # Needed for checkout, although not strictly used by the script itself
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Fetch All Project Items
        id: fetch_items
        env:
          # Use a Classic PAT stored as a secret. Needs 'repo' and 'project' scopes.
          GH_TOKEN: ${{ secrets.GH_PAT_PROJECTS }}
          PROJECT_ID: ${{ inputs.project_node_id }}
          OUTPUT_FILENAME: ${{ inputs.output_filename }}
        run: |
          echo "Starting fetch for Project ID: $PROJECT_ID"

          # Define the GraphQL query with a variable for the cursor
          read -r -d '' query << EOF
          query GetProjectItemsPaginated(\$projectId: ID!, \$itemsCursor: String) {
            node(id: \$projectId) {
              ... on ProjectV2 {
                id
                title
                number
                items(first: 100, after: \$itemsCursor) { # Max 'first' is 100, use 'after'
                  nodes {
                    id # Project Item Node ID
                    # --- Get ProjectV2 Field Values for this item ---
                    fieldValues(first: 20) {
                      nodes {
                        ... on ProjectV2ItemFieldTextValue { text field { ... on ProjectV2Field { name id } } }
                        ... on ProjectV2ItemFieldDateValue { date field { ... on ProjectV2Field { name id } } }
                        ... on ProjectV2ItemFieldNumberValue { number field { ... on ProjectV2Field { name id } } }
                        ... on ProjectV2ItemFieldSingleSelectValue { name optionId field { ... on ProjectV2SingleSelectField { name id } } }
                        ... on ProjectV2ItemFieldIterationValue { title startDate duration iterationId field { ... on ProjectV2IterationField { name id } } }
                      }
                    }
                    # --- Get details about the content (if it's an Issue) ---
                    content {
                      ... on Issue {
                        id number title state createdAt updatedAt closedAt url
                        author { login }
                        assignees(first: 10) { nodes { login } }
                        labels(first: 20) { nodes { name color } }
                        milestone { title number state }
                        issueType { name }
                        parent { ... on Issue { title number url id issueType { name } } }
                        repository { nameWithOwner }
                      }
                      # Add fragments for other content types if needed (PRs, Drafts)
                      # ... on PullRequest { ... }
                      # ... on DraftIssue { ... }
                    }
                  }
                  pageInfo {
                    endCursor
                    hasNextPage
                  }
                }
              }
            }
          }
          EOF

          # Initialize variables for pagination
          all_nodes_json='[]' # Start with an empty JSON array for all nodes
          cursor="null"       # Start with null cursor for the first page
          has_next_page=true  # Assume there's at least one page
          page_count=0
          max_pages=200       # Safety break to prevent infinite loops

          echo "Starting pagination loop..."

          while $has_next_page; do
            page_count=$((page_count + 1))
            echo "Fetching page $page_count with cursor: $cursor"

            # Execute the query for the current page
            # Use --argjson for cursor to correctly handle the 'null' string as JSON null
            if ! gh api graphql -F projectId="$PROJECT_ID" -f query="$query" --argjson itemsCursor "$cursor" > page_data.json; then
              echo "ERROR: 'gh api graphql' command failed on page $page_count."
              if [[ -s page_data.json ]]; then
                echo "API Response:"
                jq . page_data.json || cat page_data.json
              else
                echo "No response received from the API."
              fi
              exit 1
            fi

            # Validate basic response structure
            if ! jq -e '.data.node.items' page_data.json > /dev/null; then
               echo "ERROR: '.data.node.items' not found in GraphQL response on page $page_count."
               echo "--- Start of page_data.json content ---"
               jq . page_data.json || cat page_data.json
               echo "--- End of page_data.json content ---"
               exit 1
            fi

            # Extract nodes from the current page
            current_page_nodes=$(jq -c '.data.node.items.nodes // []' page_data.json)

            # Extract pageInfo
            page_info=$(jq -c '.data.node.items.pageInfo // {"hasNextPage": false, "endCursor": null}' page_data.json)
            has_next_page=$(echo "$page_info" | jq -r '.hasNextPage')
            new_cursor=$(echo "$page_info" | jq -r '.endCursor // "null"') # Get new cursor, default to "null" string if missing

            # Append current page nodes to the main list using jq's stream/add capability
            # Reads the existing list, then the new list, and sums them (array concatenation)
            all_nodes_json=$(jq -s '.[0] + .[1]' <(echo "$all_nodes_json") <(echo "$current_page_nodes"))

            # Update cursor for the next iteration - ensure it's quoted for --argjson
            cursor="\"$new_cursor\""

            echo "Page $page_count fetched. hasNextPage: $has_next_page. New cursor: $new_cursor"

            # Safety break
            if [[ $page_count -ge $max_pages ]]; then
              echo "WARNING: Reached maximum page limit ($max_pages). Stopping pagination."
              break
            fi

            # Break loop if hasNextPage is false
            if [[ "$has_next_page" != "true" ]]; then
              echo "No more pages found."
              break
            fi
          done

          echo "Pagination complete. Total pages fetched: $page_count"

          # Construct the final JSON object embedding the aggregated nodes
          # We use the structure of the first page's response as a template, but replace items.nodes
          # Get the base structure from the first page (or last page if only one)
          base_structure=$(jq '.data' page_data.json)
          # Inject the aggregated nodes array into the structure
          final_json=$(echo "$base_structure" | jq --argjson nodes "$all_nodes_json" '.node.items.nodes = $nodes')
          # Wrap it back in the top-level "data" key
          final_output=$(jq -n --argjson data "$final_json" '{"data": $data}')


          # Save the final aggregated JSON to the output file
          echo "$final_output" > "$OUTPUT_FILENAME"

          echo "Successfully fetched all items and saved to $OUTPUT_FILENAME"
          echo "output_path=$OUTPUT_FILENAME" >> $GITHUB_OUTPUT

      - name: Upload Full JSON Report Artifact
        uses: actions/upload-artifact@v4
        with:
          name: full-project-data # Name of the artifact zip file
          path: ${{ steps.fetch_items.outputs.output_path }} # Path to the generated JSON file
