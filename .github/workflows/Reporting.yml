name: Generate Project Issues Report by Node ID

on:
  workflow_dispatch:
    inputs:
      project_node_id:
        description: 'The GraphQL Node ID of the ProjectV2 (e.g., PVT_...) '
        required: true
        type: string
      repo_url:
        description: 'Full URL of the repository where the workflow runs (e.g., https://github.com/MyOrg/MyRepo)'
        required: true
        type: string

jobs:
  generate_report:
    runs-on: ubuntu-latest
    permissions:
      issues: read     # Needed for reading issue details via API
      contents: read   # Needed for actions/checkout
      # read:project scope must be granted via Org/Repo Settings or PAT
    env:
      # Pass Project Node ID directly to the script
      PROJECT_NODE_ID: ${{ github.event.inputs.project_node_id }}
      # --- Authentication: Choose ONE ---
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} # Default - Requires project access granted in Org/Repo settings
      # GITHUB_TOKEN: ${{ secrets.PROJECT_REPORT_PAT }} # Fallback - PAT needs 'read:project' & 'repo' scopes

    steps:
      - name: Parse Repository Owner and Name
        id: repo_info
        run: |
          # Extracts "owner/repo" from "https://github.com/owner/repo" or "http://github.com/owner/repo/"
          repo_full_name=$(echo "${{ github.event.inputs.repo_url }}" | sed -E 's|https?://github.com/||; s|/$||')
          echo "repo_full_name=$repo_full_name" >> $GITHUB_OUTPUT
          echo "Parsed repository: $repo_full_name"

      - name: Checkout repository code
        uses: actions/checkout@v4
        with:
          # Use the parsed owner/repo name
          repository: ${{ steps.repo_info.outputs.repo_full_name }}
          # If using a PAT for API calls AND checkout fails (e.g., private repo),
          # you might need to explicitly pass the PAT here too. Ensure PAT has 'repo' scope.
          # token: ${{ secrets.PROJECT_REPORT_PAT }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: python -m pip install requests

      - name: Generate Project Report Script
        id: generate_script
        # Script generation uses the job-level env vars (PROJECT_NODE_ID, GITHUB_TOKEN)
        run: |
          cat << 'EOF' > generate_report.py
          # Python script to fetch GitHub Project issues and generate a CSV report
          # Uses Project Node ID directly.
          import os
          import sys
          import requests
          import csv
          import json
          from datetime import datetime, timezone

          # --- Configuration ---
          PROJECT_NODE_ID = os.environ.get("PROJECT_NODE_ID") # Get Node ID from env
          GITHUB_TOKEN = os.environ.get("GITHUB_TOKEN")
          API_URL = "https://api.github.com/graphql"
          CSV_FILENAME = "project_issues_report.csv"
          PAGE_SIZE = 100

          # --- Input Validation ---
          if not PROJECT_NODE_ID:
              print("Error: Missing 'PROJECT_NODE_ID' environment variable.", file=sys.stderr)
              sys.exit(1)
          if not GITHUB_TOKEN:
              print("Error: GITHUB_TOKEN env var not found.", file=sys.stderr)
              sys.exit(1)
          # Basic check for plausible Node ID format (optional but helpful)
          if not ("_" in PROJECT_NODE_ID and len(PROJECT_NODE_ID) > 10):
               print(f"Warning: Provided PROJECT_NODE_ID ('{PROJECT_NODE_ID}') doesn't look like a standard Node ID.", file=sys.stderr)


          # --- Field Definitions --- (Same as before)
          CUSTOM_FIELD_NAMES = {
              "sprint": "Sprint", "team": "Team", "priority": "Priority", "severity": "Severity",
              "effort": "Effort", "status": "Status", "t_shirt_size": "T-Shirt Size", "blocked": "Blocked?"
          }
          CSV_HEADERS = [
              "Title", "Issue Number", "Type", "Parent Issue Number", "Sprint", "Milestone",
              "Created_at", "Closed_at", "Team", "Priority", "Severity", "Effort",
              "Status", "Labels", "T-Shirt Size", "Blocked?"
          ]

          # --- Helper Functions --- (run_graphql_query is same, get_project_node_id is removed)
          def run_graphql_query(query, variables=None):
              # (Function content is identical to previous version - handles API call)
              headers = { "Authorization": f"bearer {GITHUB_TOKEN}", "Content-Type": "application/json", "GraphQL-Features": "issue_types" }
              payload = {"query": query}
              if variables: payload["variables"] = variables
              try:
                  response = requests.post(API_URL, headers=headers, json=payload, timeout=30)
                  response.raise_for_status()
              except requests.exceptions.Timeout: print("Error: GraphQL request timed out.", file=sys.stderr); sys.exit(1)
              except requests.exceptions.RequestException as e:
                  print(f"Error: Network/Request error during GraphQL query: {e}", file=sys.stderr)
                  if e.response is not None: print(f"Response Status: {e.response.status_code}\nResponse Text: {e.response.text[:500]}...", file=sys.stderr)
                  sys.exit(1)
              result = response.json()
              if "errors" in result:
                  print("Error: GraphQL API returned errors:", file=sys.stderr); print(json.dumps(result["errors"], indent=2), file=sys.stderr)
                  error_message = result['errors'][0].get('message', 'Unknown GraphQL error'); error_type = result['errors'][0].get('type')
                  # Updated Hints
                  if "Could not resolve to a node with the global ID" in error_message or error_type == "NOT_FOUND": print(f"Hint: Verify PROJECT_NODE_ID ('{PROJECT_NODE_ID}'). Ensure token has access.", file=sys.stderr)
                  elif "FIELD_ACCESS_DENIED" in str(result["errors"]): print("Hint: Token missing scopes/permissions? (read:project, repo, read:org). Check Org/Repo Action settings or PAT scopes.", file=sys.stderr)
                  elif "API rate limit exceeded" in error_message: print("Hint: Hit API rate limit.", file=sys.stderr)
                  sys.exit(f"GraphQL query failed: {error_message}")
              return result.get("data", {})

          # --- get_project_node_id function REMOVED ---

          def format_iso_date(date_string):
              # (Function content is identical to previous version)
              if not date_string: return ""
              try:
                  dt = datetime.fromisoformat(date_string.replace('Z', '+00:00')) if date_string.endswith('Z') else datetime.fromisoformat(date_string)
                  if dt.tzinfo is None or dt.tzinfo.utcoffset(dt) is None: dt = dt.replace(tzinfo=timezone.utc)
                  return dt.astimezone(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')
              except ValueError: print(f"Warning: Could not parse date '{date_string}'.", file=sys.stderr); return date_string

          # --- Main GraphQL Query for Project Items --- (Same as before)
          PROJECT_ITEMS_QUERY = """
          query GetProjectItems($projectId: ID!, $firstItems: Int!, $afterItemCursor: String, $firstFields: Int!) {
            node(id: $projectId) { ... on ProjectV2 {
                items(first: $firstItems, after: $afterItemCursor, orderBy: {field: POSITION, direction: ASC}) {
                  totalCount pageInfo { endCursor hasNextPage }
                  nodes { id createdAt content { __typename
                      ... on Issue { id number title url state createdAt closedAt milestone { title } labels(first: 20) { nodes { name } } issueType { name } }
                      ... on PullRequest { id number title url } }
                    fieldValues(first: $firstFields) { nodes { __typename
                        ... on ProjectV2ItemFieldTextValue { text field { ... on ProjectV2FieldCommon { name } } }
                        ... on ProjectV2ItemFieldSingleSelectValue { name field { ... on ProjectV2FieldCommon { name } } }
                        ... on ProjectV2ItemFieldDateValue { date field { ... on ProjectV2FieldCommon { name } } }
                        ... on ProjectV2ItemFieldIterationValue { title field { ... on ProjectV2FieldCommon { name } } }
                        ... on ProjectV2ItemFieldNumberValue { number field { ... on ProjectV2FieldCommon { name } } }
                        ... on ProjectV2ItemFieldValueCommon { field { ... on ProjectV2FieldCommon { name } } } }}}}}}}
          fragment ProjectV2FieldCommon on ProjectV2Field { name }
          """

          # --- Main Execution Logic ---
          # Use the PROJECT_NODE_ID directly from the environment variable
          project_node_id = PROJECT_NODE_ID
          print(f"\nStarting report generation for project with Node ID: {project_node_id}...")
          # Removed call to get_project_node_id

          all_issues_data = []; found_custom_fields = set(); expected_custom_field_names = set(CUSTOM_FIELD_NAMES.values())
          has_next_page = True; current_cursor = None; print("\nFetching project items..."); item_count = 0; total_items = 0
          while has_next_page:
              print(f"  Fetching page starting with cursor: {current_cursor or 'None'}")
              # Use the project_node_id variable which holds the direct ID
              variables = { "projectId": project_node_id, "firstItems": PAGE_SIZE, "afterItemCursor": current_cursor, "firstFields": 50 }
              try: data = run_graphql_query(PROJECT_ITEMS_QUERY, variables)
              except SystemExit: print("Exiting due to error during item fetch.", file=sys.stderr); has_next_page = False; break
              except Exception as e: print(f"\nCritical Error during GraphQL query: {e}", file=sys.stderr); sys.exit(1)

              # --- Rest of the item processing loop is identical to previous version ---
              project_data = data.get("node", {});
              if not project_data:
                   # Add check if the node is not a ProjectV2, indicating wrong ID type was passed
                   node_type = data.get("node", {}).get("__typename")
                   if node_type: print(f"Error: Node ID '{project_node_id}' resolved to type '{node_type}', expected 'ProjectV2'.", file=sys.stderr)
                   else: print(f"Error: Could not resolve Node ID '{project_node_id}'. Check ID and token permissions.", file=sys.stderr)
                   sys.exit(1) # Exit if node isn't found or isn't a project
              items_data = project_data.get("items", {}); nodes = items_data.get("nodes", []); page_info = items_data.get("pageInfo", {}); current_total = items_data.get("totalCount", 0)
              if current_cursor is None or current_total > total_items:
                   total_items = current_total
                   if current_cursor is None: print(f"  Project reports {total_items} total items.")
              if not nodes:
                  if current_cursor is None and total_items == 0: print("  Project has no items.")
                  elif total_items > item_count: print(f"Warning: Received empty item list unexpectedly ({item_count}/{total_items}). Stopping.", file=sys.stderr)
                  break
              page_item_count = 0
              for item in nodes:
                  item_count += 1; page_item_count += 1; content = item.get("content")
                  if not content or content.get("__typename") != "Issue": continue
                  issue_details = {header: "" for header in CSV_HEADERS}
                  issue_details["Title"] = content.get("title", ""); issue_details["Issue Number"] = content.get("number", "")
                  issue_details["Type"] = content.get("issueType", {}).get("name", "")
                  issue_details["Milestone"] = content.get("milestone", {}).get("title", "") if content.get("milestone") else ""
                  issue_details["Created_at"] = format_iso_date(content.get("createdAt", ""))
                  issue_details["Closed_at"] = format_iso_date(content.get("closedAt", ""))
                  labels_list = [label.get("name") for label in content.get("labels", {}).get("nodes", []) if label.get("name")]
                  issue_details["Labels"] = ", ".join(labels_list)
                  field_values = item.get("fieldValues", {}).get("nodes", [])
                  for field_value in field_values:
                      field_node = field_value.get("field", {}); field_name = field_node.get("name") if field_node else None
                      if not field_name: continue
                      if field_name in expected_custom_field_names: found_custom_fields.add(field_name)
                      if field_name == CUSTOM_FIELD_NAMES.get("sprint"): issue_details["Sprint"] = field_value.get("title", "")
                      elif field_name == CUSTOM_FIELD_NAMES.get("team"): issue_details["Team"] = field_value.get("name", "")
                      elif field_name == CUSTOM_FIELD_NAMES.get("priority"): issue_details["Priority"] = field_value.get("name", "")
                      elif field_name == CUSTOM_FIELD_NAMES.get("severity"): issue_details["Severity"] = field_value.get("name", "")
                      elif field_name == CUSTOM_FIELD_NAMES.get("effort"): issue_details["Effort"] = field_value.get("number", "")
                      elif field_name == CUSTOM_FIELD_NAMES.get("status"): issue_details["Status"] = field_value.get("name", "")
                      elif field_name == CUSTOM_FIELD_NAMES.get("t_shirt_size"): issue_details["T-Shirt Size"] = field_value.get("name", "")
                      elif field_name == CUSTOM_FIELD_NAMES.get("blocked"): issue_details["Blocked?"] = field_value.get("name", "")
                  all_issues_data.append(issue_details)
              print(f"  Processed {page_item_count} items from page ({item_count}/{total_items} total).")
              has_next_page = page_info.get("hasNextPage", False); current_cursor = page_info.get("endCursor") if has_next_page else None
              if not has_next_page: print(f"\nFinished fetching all {total_items} items."); break

          # --- Warnings and CSV Writing --- (Same as before)
          missing_fields = expected_custom_field_names - found_custom_fields
          if missing_fields:
              print("\n--- Warnings ---", file=sys.stderr); print("Following expected custom fields NOT found:", file=sys.stderr)
              for field in sorted(list(missing_fields)): print(f"  - '{field}'", file=sys.stderr)
              print("Corresponding CSV columns may be empty. Verify names (case-sensitive).", file=sys.stderr); print("---------------\n")
          print(f"\nWriting {len(all_issues_data)} issues to {CSV_FILENAME}...")
          if not all_issues_data: print("Warning: No issue data collected. CSV will contain headers only." if total_items > 0 else "No issue data collected. CSV has headers only.", file=sys.stderr)
          try:
              with open(CSV_FILENAME, 'w', newline='', encoding='utf-8') as csvfile:
                  writer = csv.DictWriter(csvfile, fieldnames=CSV_HEADERS, extrasaction='ignore'); writer.writeheader()
                  if all_issues_data: writer.writerows(all_issues_data)
              print(f"Successfully wrote data to {CSV_FILENAME}")
          except IOError as e: print(f"\nError: Failed to write CSV file '{CSV_FILENAME}': {e}", file=sys.stderr); sys.exit(1)
          print("\nScript finished successfully.")

          EOF
          echo "Python script 'generate_report.py' created."

      - name: Run Report Generation Script
        run: python generate_report.py

      - name: Upload Report Artifact
        uses: actions/upload-artifact@v4
        with:
          # Using Node ID in artifact name might be long, but it's specific
          name: project-issues-report-${{ github.event.inputs.project_node_id }}
          path: project_issues_report.csv
          if-no-files-found: warn
          retention-days: 7
